"""
Auto Scrape - T·ª± ƒë·ªông ki·ªÉm tra v√† scrape chapters m·ªõi

Script n√†y s·∫Ω:
1. Qu√©t folder Chapters_Untranslated ƒë·ªÉ t√¨m chapter cao nh·∫•t ƒë√£ c√≥
2. T·ª± ƒë·ªông scrape ƒë·∫øn chapter m·ªõi nh·∫•t (ho·∫∑c theo target)
3. L∆∞u v√†o folder Chapters_Untranslated

C√°ch s·ª≠ d·ª•ng:
    # T·ª± ƒë·ªông scrape T·∫§T C·∫¢ chapters m·ªõi (ƒë·∫øn khi h·∫øt)
    python auto_scrape.py --auto --url "https://ko-fi.com/post/..."
    
    # Ch·ªâ ƒë·ªãnh chapter ƒë√≠ch
    python auto_scrape.py --target 270 --url "..."
    
    # Ch·ªâ ƒë·ªãnh s·ªë b√†i vi·∫øt
    python auto_scrape.py --count 5 --url "..."
    
    # Xem tr·∫°ng th√°i
    python auto_scrape.py --status
"""

import os
import re
import sys
import argparse
from pathlib import Path

# Import t·ª´ kofi_scraper_fast
try:
    from kofi_scraper_fast import FastKofiScraper, parse_chapters_from_content, format_to_xml
    import asyncio
except ImportError:
    print("‚ùå Kh√¥ng th·ªÉ import kofi_scraper_fast. ƒê·∫£m b·∫£o file n·∫±m c√πng th∆∞ m·ª•c.")
    sys.exit(1)


# C·∫•u h√¨nh
CHAPTERS_DIR = Path(__file__).parent / "Chapters_Untranslated"
# Base URL pattern cho Ko-fi posts (c·∫ßn ƒëi·ªÅn ƒë√∫ng author)
KOFI_AUTHOR = "your_kofi_author"  # Thay ƒë·ªïi n·∫øu c·∫ßn


def save_chapters_separately(chapters: list, output_dir: Path = None) -> list:
    """
    L∆∞u t·ª´ng chapter v√†o file ri√™ng bi·ªát.
    
    Args:
        chapters: List c√°c chapter dict v·ªõi keys: id, title, content
        output_dir: Th∆∞ m·ª•c l∆∞u file (m·∫∑c ƒë·ªãnh: CHAPTERS_DIR)
    
    Returns:
        Danh s√°ch c√°c ƒë∆∞·ªùng d·∫´n file ƒë√£ t·∫°o
    """
    if not chapters:
        return []
    
    if output_dir is None:
        output_dir = CHAPTERS_DIR
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    saved_files = []
    
    for chapter in chapters:
        ch_id = chapter['id']
        filename = f"ch{ch_id}.txt"
        output_path = output_dir / filename
        
        # Format single chapter to XML
        xml_content = format_to_xml([chapter])
        output_path.write_text(xml_content, encoding='utf-8')
        
        saved_files.append(output_path)
        print(f"   üíæ ƒê√£ l∆∞u: {filename}")
    
    return saved_files


def get_latest_chapter() -> int:
    """Qu√©t folder v√† tr·∫£ v·ªÅ s·ªë chapter cao nh·∫•t ƒë√£ c√≥"""
    if not CHAPTERS_DIR.exists():
        print(f"‚ö†Ô∏è Folder {CHAPTERS_DIR} kh√¥ng t·ªìn t·∫°i. T·∫°o m·ªõi...")
        CHAPTERS_DIR.mkdir(parents=True, exist_ok=True)
        return 0
    
    max_chapter = 0
    chapter_files = list(CHAPTERS_DIR.glob("ch*.txt"))
    
    for file in chapter_files:
        # Parse filename: ch255_257.txt ho·∫∑c ch258.txt
        match = re.match(r'ch(\d+)(?:_(\d+))?\.txt', file.name)
        if match:
            start_ch = int(match.group(1))
            end_ch = int(match.group(2)) if match.group(2) else start_ch
            max_chapter = max(max_chapter, end_ch)
    
    return max_chapter


def get_chapter_summary() -> dict:
    """Tr·∫£ v·ªÅ th√¥ng tin t·ªïng h·ª£p v·ªÅ c√°c chapters ƒë√£ c√≥"""
    if not CHAPTERS_DIR.exists():
        return {"files": [], "total_chapters": 0, "latest": 0, "gaps": []}
    
    files = []
    all_chapters = set()
    
    for file in sorted(CHAPTERS_DIR.glob("ch*.txt")):
        match = re.match(r'ch(\d+)(?:_(\d+))?\.txt', file.name)
        if match:
            start_ch = int(match.group(1))
            end_ch = int(match.group(2)) if match.group(2) else start_ch
            files.append({
                "name": file.name,
                "start": start_ch,
                "end": end_ch,
                "size": file.stat().st_size
            })
            for ch in range(start_ch, end_ch + 1):
                all_chapters.add(ch)
    
    # T√¨m gaps (chapters b·ªã thi·∫øu)
    gaps = []
    if all_chapters:
        min_ch = min(all_chapters)
        max_ch = max(all_chapters)
        for ch in range(min_ch, max_ch + 1):
            if ch not in all_chapters:
                gaps.append(ch)
    
    return {
        "files": files,
        "total_chapters": len(all_chapters),
        "latest": max(all_chapters) if all_chapters else 0,
        "gaps": gaps
    }


def print_status():
    """In tr·∫°ng th√°i hi·ªán t·∫°i c·ªßa chapters"""
    summary = get_chapter_summary()
    
    print("\n" + "=" * 60)
    print("üìö TR·∫†NG TH√ÅI CHAPTERS")
    print("=" * 60)
    
    if not summary["files"]:
        print("   Ch∆∞a c√≥ chapter n√†o trong folder.")
    else:
        print(f"\nüìÅ Folder: {CHAPTERS_DIR}")
        print(f"\nüìñ Files hi·ªán c√≥:")
        for f in summary["files"]:
            size_kb = f["size"] / 1024
            print(f"   ‚Ä¢ {f['name']} (Ch.{f['start']}-{f['end']}) - {size_kb:.1f}KB")
        
        print(f"\nüìä T·ªïng h·ª£p:")
        print(f"   ‚Ä¢ T·ªïng s·ªë chapters: {summary['total_chapters']}")
        print(f"   ‚Ä¢ Chapter m·ªõi nh·∫•t: {summary['latest']}")
        
        if summary["gaps"]:
            print(f"   ‚Ä¢ ‚ö†Ô∏è Chapters b·ªã thi·∫øu: {summary['gaps']}")
    
    print("=" * 60)
    return summary


async def scrape_new_chapters(start_url: str, count: int, delay_ms: int = 500):
    """Scrape chapters m·ªõi s·ª≠ d·ª•ng FastKofiScraper"""
    scraper = FastKofiScraper(
        debug_port=9222,
        parallel_tabs=1,  # Sequential ƒë·ªÉ follow Next Chapter link
        delay_ms=delay_ms
    )
    
    chapters = await scraper.scrape_sequential_with_next(start_url, count)
    
    if chapters:
        saved_files = save_chapters_separately(chapters, CHAPTERS_DIR)
        return chapters, saved_files
    
    return [], []


async def scrape_until_end(start_url: str, delay_ms: int = 500, max_posts: int = 50):
    """
    T·ª± ƒë·ªông scrape T·∫§T C·∫¢ chapters m·ªõi cho ƒë·∫øn khi h·∫øt Next Chapter link.
    
    Args:
        start_url: URL b·∫Øt ƒë·∫ßu
        delay_ms: Delay gi·ªØa c√°c request
        max_posts: Gi·ªõi h·∫°n s·ªë b√†i t·ªëi ƒëa ƒë·ªÉ tr√°nh loop v√¥ h·∫°n (m·∫∑c ƒë·ªãnh: 50)
    """
    scraper = FastKofiScraper(
        debug_port=9222,
        parallel_tabs=1,
        delay_ms=delay_ms
    )
    
    all_chapters = []
    current_url = start_url
    post_count = 0
    
    # L·∫•y tab hi·ªán c√≥
    tabs = await scraper.get_tabs()
    kofi_tab = None
    for tab in tabs:
        if tab.get('type') == 'page':
            kofi_tab = tab
            break
    
    if not kofi_tab:
        raise Exception("Kh√¥ng t√¨m th·∫•y tab Chrome n√†o")
    
    tab_id = kofi_tab['id']
    
    try:
        await scraper.connect_to_tab(kofi_tab)
        
        while current_url and post_count < max_posts:
            post_count += 1
            print(f"\nüìñ [B√†i {post_count}] Scraping: {current_url[:60]}...")
            
            # Navigate v√† ƒë·ª£i
            loaded = await scraper.navigate_and_wait(tab_id, current_url)
            if not loaded:
                print("   ‚ö†Ô∏è Page load timeout, th·ª≠ extract anyway...")
            
            # Extract
            data = await scraper.extract_chapter_from_tab(tab_id)
            chapters = parse_chapters_from_content(data['content'], data['title'])
            
            all_chapters.extend(chapters)
            
            for ch in chapters:
                print(f"   ‚úÖ Chapter {ch['id']}: {ch['title'][:40]}...")
            
            # T√¨m Next URL
            next_url = data.get('nextChapterUrl')
            
            if next_url and next_url != current_url:
                current_url = next_url
                await asyncio.sleep(delay_ms / 1000)
            else:
                print("\nüèÅ ƒê√£ ƒë·∫øn chapter m·ªõi nh·∫•t! (Kh√¥ng c√≥ Next Chapter link)")
                break
                
    finally:
        if tab_id in scraper.sessions:
            await scraper.sessions[tab_id]['ws'].close()
    
    # L∆∞u t·ª´ng chapter v√†o file ri√™ng
    if all_chapters:
        saved_files = save_chapters_separately(all_chapters, CHAPTERS_DIR)
        return all_chapters, saved_files
    
    return [], []


def main():
    parser = argparse.ArgumentParser(
        description='T·ª± ƒë·ªông ki·ªÉm tra v√† scrape chapters m·ªõi t·ª´ Ko-fi',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('--url', '-u', 
                        help='URL c·ªßa chapter ti·∫øp theo c·∫ßn scrape')
    parser.add_argument('--target', '-t', type=int,
                        help='Chapter ƒë√≠ch (m·∫∑c ƒë·ªãnh: h·ªèi user)')
    parser.add_argument('--count', '-c', type=int,
                        help='S·ªë b√†i vi·∫øt c·∫ßn scrape (thay th·∫ø --target)')
    parser.add_argument('--auto', '-a', action='store_true',
                        help='T·ª± ƒë·ªông scrape T·∫§T C·∫¢ chapters m·ªõi ƒë·∫øn khi h·∫øt')
    parser.add_argument('--delay', '-d', type=int, default=500,
                        help='Delay gi·ªØa c√°c request (ms, m·∫∑c ƒë·ªãnh: 500)')
    parser.add_argument('--max', '-m', type=int, default=50,
                        help='Gi·ªõi h·∫°n s·ªë b√†i t·ªëi ƒëa khi d√πng --auto (m·∫∑c ƒë·ªãnh: 50)')
    parser.add_argument('--yes', '-y', action='store_true',
                        help='B·ªè qua x√°c nh·∫≠n, b·∫Øt ƒë·∫ßu scrape ngay')
    parser.add_argument('--status', '-s', action='store_true',
                        help='Ch·ªâ hi·ªán tr·∫°ng th√°i, kh√¥ng scrape')
    
    args = parser.parse_args()
    
    # Hi·ªÉn th·ªã banner
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë             AUTO SCRAPE - Ko-fi Chapter Scraper              ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë T·ª± ƒë·ªông ki·ªÉm tra v√† scrape chapters m·ªõi                      ‚ïë
‚ïë                                                              ‚ïë
‚ïë Y√™u c·∫ßu: Chrome ch·∫°y v·ªõi --remote-debugging-port=9222        ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # Hi·ªÉn th·ªã tr·∫°ng th√°i
    summary = print_status()
    latest_chapter = summary["latest"]
    
    if args.status:
        return
    
    # Ch·∫ø ƒë·ªô AUTO - scrape t·∫•t c·∫£ ƒë·∫øn khi h·∫øt
    if args.auto:
        if not args.url:
            print("\nüìé Nh·∫≠p URL c·ªßa chapter ti·∫øp theo c·∫ßn scrape:")
            print(f"   (Chapter ti·∫øp theo sau Ch.{latest_chapter})")
            args.url = input("   URL: ").strip()
            
            if not args.url:
                print("   ‚ùå C·∫ßn c√≥ URL ƒë·ªÉ scrape")
                return
        
        print(f"\nüöÄ CH·∫æ ƒê·ªò T·ª∞ ƒê·ªòNG - Scrape ƒë·∫øn chapter m·ªõi nh·∫•t!")
        print(f"   ‚Ä¢ URL: {args.url[:60]}...")
        print(f"   ‚Ä¢ Delay: {args.delay}ms")
        print(f"   ‚Ä¢ Gi·ªõi h·∫°n: {args.max} b√†i")
        
        if not args.yes:
            confirm = input("\n   B·∫Øt ƒë·∫ßu? (y/N): ").strip().lower()
            if confirm != 'y':
                print("   ƒê√£ h·ªßy.")
                return
        
        print("\n‚è≥ ƒêang scrape t·ª± ƒë·ªông...")
        try:
            chapters, output_path = asyncio.run(
                scrape_until_end(args.url, args.delay, args.max)
            )
            
            if chapters:
                print(f"\n‚úÖ HO√ÄN TH√ÄNH!")
                print(f"   üìñ ƒê√£ scrape {len(chapters)} chapters")
                print(f"   üìÅ ƒê√£ l∆∞u {len(output_path)} files v√†o: {CHAPTERS_DIR}")
                print_status()
            else:
                print("\n‚ùå Kh√¥ng scrape ƒë∆∞·ª£c chapter n√†o")
                
        except Exception as e:
            print(f"\n‚ùå L·ªói: {e}")
            import traceback
            traceback.print_exc()
        
        return
    
    # Ch·∫ø ƒë·ªô th∆∞·ªùng - ch·ªâ ƒë·ªãnh count ho·∫∑c target
    if args.count:
        count = args.count
        print(f"\nüéØ S·∫Ω scrape {count} b√†i vi·∫øt ti·∫øp theo")
    elif args.target:
        if args.target <= latest_chapter:
            print(f"\n‚ö†Ô∏è Chapter {args.target} ƒë√£ c√≥ r·ªìi!")
            return
        chapters_needed = args.target - latest_chapter
        count = max(1, chapters_needed // 2)
        print(f"\nüéØ C·∫ßn scrape kho·∫£ng {count} b√†i ƒë·ªÉ ƒë·∫øn Chapter {args.target}")
    else:
        # H·ªèi user
        print(f"\n‚ùì Chapter m·ªõi nh·∫•t hi·ªán c√≥: {latest_chapter}")
        print("   T√πy ch·ªçn:")
        print("   ‚Ä¢ Nh·∫≠p s·ªë chapter ƒë√≠ch (VD: 280)")
        print("   ‚Ä¢ Nh·∫≠p 'auto' ƒë·ªÉ scrape t·∫•t c·∫£ chapters m·ªõi")
        print("   ‚Ä¢ Enter ƒë·ªÉ tho√°t")
        
        try:
            choice = input("\n   L·ª±a ch·ªçn: ").strip().lower()
            if not choice:
                print("   ƒê√£ b·ªè qua.")
                return
            
            if choice == 'auto':
                # Chuy·ªÉn sang ch·∫ø ƒë·ªô auto
                args.auto = True
                if not args.url:
                    print("\nüìé Nh·∫≠p URL c·ªßa chapter ti·∫øp theo:")
                    args.url = input("   URL: ").strip()
                    if not args.url:
                        print("   ‚ùå C·∫ßn c√≥ URL")
                        return
                
                print("\n‚è≥ ƒêang scrape t·ª± ƒë·ªông...")
                chapters, output_path = asyncio.run(
                    scrape_until_end(args.url, args.delay, args.max)
                )
                
                if chapters:
                    print(f"\n‚úÖ HO√ÄN TH√ÄNH!")
                    print(f"   üìñ ƒê√£ scrape {len(chapters)} chapters")
                    print(f"   üìÅ ƒê√£ l∆∞u {len(output_path)} files v√†o: {CHAPTERS_DIR}")
                    print_status()
                return
            
            target = int(choice)
            if target <= latest_chapter:
                print(f"   ‚ö†Ô∏è Chapter {target} ƒë√£ c√≥ r·ªìi!")
                return
            chapters_needed = target - latest_chapter
            count = max(1, chapters_needed // 2)
            print(f"   üìù S·∫Ω scrape kho·∫£ng {count} b√†i vi·∫øt")
        except ValueError:
            print("   ‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá")
            return
    
    # Y√™u c·∫ßu URL n·∫øu ch∆∞a c√≥
    if not args.url:
        print("\nüìé Nh·∫≠p URL c·ªßa chapter ti·∫øp theo c·∫ßn scrape:")
        print(f"   (Chapter ti·∫øp theo sau Ch.{latest_chapter})")
        args.url = input("   URL: ").strip()
        
        if not args.url:
            print("   ‚ùå C·∫ßn c√≥ URL ƒë·ªÉ scrape")
            return
        
        if "ko-fi.com" not in args.url:
            print("   ‚ö†Ô∏è URL kh√¥ng ph·∫£i Ko-fi, ti·∫øp t·ª•c anyway...")
    
    # X√°c nh·∫≠n
    print(f"\nüöÄ S·∫µn s√†ng scrape:")
    print(f"   ‚Ä¢ URL: {args.url[:60]}...")
    print(f"   ‚Ä¢ S·ªë b√†i: {count}")
    print(f"   ‚Ä¢ Delay: {args.delay}ms")
    
    if not args.yes:
        confirm = input("\n   B·∫Øt ƒë·∫ßu? (y/N): ").strip().lower()
        if confirm != 'y':
            print("   ƒê√£ h·ªßy.")
            return
    
    # Th·ª±c hi·ªán scrape
    print("\n‚è≥ ƒêang scrape...")
    try:
        chapters, output_path = asyncio.run(
            scrape_new_chapters(args.url, count, args.delay)
        )
        
        if chapters:
            print(f"\n‚úÖ HO√ÄN TH√ÄNH!")
            print(f"   üìñ ƒê√£ scrape {len(chapters)} chapters")
            print(f"   üìÅ ƒê√£ l∆∞u {len(output_path)} files v√†o: {CHAPTERS_DIR}")
            
            # Hi·ªÉn th·ªã tr·∫°ng th√°i m·ªõi
            print_status()
        else:
            print("\n‚ùå Kh√¥ng scrape ƒë∆∞·ª£c chapter n√†o")
            
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
        print("\nüí° ƒê·∫£m b·∫£o:")
        print("   1. Chrome ƒëang ch·∫°y v·ªõi --remote-debugging-port=9222")
        print("   2. ƒê√£ ƒëƒÉng nh·∫≠p Ko-fi trong Chrome")
        print("   3. URL h·ª£p l·ªá")


if __name__ == "__main__":
    main()
