"""
Ko-fi Fast Chapter Scraper - Tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™

So vá»›i báº£n cÅ©:
- Sá»­ dá»¥ng parallel scraping (má»Ÿ nhiá»u tab Ä‘á»“ng thá»i)
- Äá»£i DOM ready thay vÃ¬ sleep cá»‘ Ä‘á»‹nh
- Giáº£m delay giá»¯a cÃ¡c request
- CÃ³ thá»ƒ scrape 5+ chapters trong vÃ i giÃ¢y

CÃ¡ch sá»­ dá»¥ng:
1. Má»Ÿ Chrome vá»›i debugging:
   chrome.exe --remote-debugging-port=9222 --user-data-dir="C:/ChromeDebug"

2. Login vÃ o Ko-fi trong Chrome

3. Cháº¡y script:
   python kofi_scraper_fast.py --urls "url1" "url2" "url3"
   hoáº·c
   python kofi_scraper_fast.py --url "start_url" --count 5
   
TÃ¹y chá»n:
   --parallel 3    Sá»‘ tab cháº¡y song song (máº·c Ä‘á»‹nh: 3)
   --delay 500     Delay giá»¯a cÃ¡c request (ms, máº·c Ä‘á»‹nh: 500)
"""

import asyncio
import argparse
import json
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple

try:
    import websockets
    import aiohttp
except ImportError:
    print("Cáº§n cÃ i Ä‘áº·t: pip install websockets aiohttp")
    exit(1)


class FastKofiScraper:
    """Scraper tá»‘i Æ°u vá»›i parallel processing"""
    
    def __init__(self, debug_port=9222, parallel_tabs=3, delay_ms=500):
        self.debug_port = debug_port
        self.parallel_tabs = parallel_tabs
        self.delay_ms = delay_ms
        self.sessions: Dict[str, dict] = {}
        self.message_counters: Dict[str, int] = {}
        
    async def get_tabs(self) -> List[dict]:
        """Láº¥y danh sÃ¡ch tabs tá»« Chrome"""
        async with aiohttp.ClientSession() as session:
            async with session.get(f'http://localhost:{self.debug_port}/json') as resp:
                return await resp.json()
    
    async def create_new_tab(self, url: str = 'about:blank') -> dict:
        """Táº¡o tab má»›i trong Chrome"""
        async with aiohttp.ClientSession() as session:
            async with session.get(
                f'http://localhost:{self.debug_port}/json/new?{url}'
            ) as resp:
                return await resp.json()
    
    async def close_tab(self, tab_id: str):
        """ÄÃ³ng tab"""
        async with aiohttp.ClientSession() as session:
            await session.get(f'http://localhost:{self.debug_port}/json/close/{tab_id}')
    
    async def connect_to_tab(self, tab: dict) -> websockets.WebSocketClientProtocol:
        """Káº¿t ná»‘i WebSocket Ä‘áº¿n tab"""
        ws_url = tab['webSocketDebuggerUrl']
        ws = await websockets.connect(ws_url)
        tab_id = tab['id']
        self.sessions[tab_id] = {'ws': ws, 'tab': tab}
        self.message_counters[tab_id] = 0
        return ws
    
    async def send_command(self, tab_id: str, method: str, params: dict = None) -> dict:
        """Gá»­i CDP command"""
        self.message_counters[tab_id] += 1
        msg_id = self.message_counters[tab_id]
        
        ws = self.sessions[tab_id]['ws']
        message = {'id': msg_id, 'method': method, 'params': params or {}}
        
        await ws.send(json.dumps(message))
        
        while True:
            response = await ws.recv()
            data = json.loads(response)
            if data.get('id') == msg_id:
                if 'error' in data:
                    raise Exception(f"CDP Error: {data['error']}")
                return data.get('result', {})
    
    async def wait_for_page_load(self, tab_id: str, timeout: float = 10.0):
        """Äá»£i page load xong - tá»‘i Æ°u hÆ¡n sleep cá»‘ Ä‘á»‹nh"""
        ws = self.sessions[tab_id]['ws']
        
        # Enable Page domain events
        await self.send_command(tab_id, 'Page.enable')
        
        start_time = asyncio.get_event_loop().time()
        
        while True:
            try:
                # Timeout ngáº¯n Ä‘á»ƒ check events
                response = await asyncio.wait_for(ws.recv(), timeout=0.5)
                data = json.loads(response)
                
                # TÃ¬m loadEventFired event
                if data.get('method') == 'Page.loadEventFired':
                    # Page Ä‘Ã£ load xong, Ä‘á»£i thÃªm chÃºt cho Shadow DOM
                    await asyncio.sleep(0.3)
                    return True
                    
            except asyncio.TimeoutError:
                pass
            
            # Check timeout tá»•ng
            if asyncio.get_event_loop().time() - start_time > timeout:
                # Timeout, nhÆ°ng váº«n tiáº¿p tá»¥c thá»­ extract
                return False
    
    async def navigate_and_wait(self, tab_id: str, url: str) -> bool:
        """Navigate Ä‘áº¿n URL vÃ  Ä‘á»£i load xong"""
        await self.send_command(tab_id, 'Page.navigate', {'url': url})
        return await self.wait_for_page_load(tab_id)
    
    async def execute_js(self, tab_id: str, expression: str):
        """Thá»±c thi JavaScript"""
        result = await self.send_command(tab_id, 'Runtime.evaluate', {
            'expression': expression,
            'returnByValue': True,
            'awaitPromise': True
        })
        
        if 'exceptionDetails' in result:
            raise Exception(f"JS Error: {result['exceptionDetails']}")
        
        return result.get('result', {}).get('value')
    
    async def extract_chapter_from_tab(self, tab_id: str) -> dict:
        """Extract content tá»« má»™t tab"""
        js_code = '''
        (() => {
            const titleElement = document.querySelector('h1') || document.querySelector('h2');
            const postTitle = titleElement ? titleElement.innerText.trim() : document.title;
            
            const articleHost = document.querySelector('.article-host');
            let bodyText = '';
            let nextUrl = null;
            
            if (articleHost && articleHost.shadowRoot) {
                const shadowRoot = articleHost.shadowRoot;
                const frView = shadowRoot.querySelector('.fr-view');
                
                bodyText = frView ? frView.innerText : (shadowRoot.textContent || '');
                
                const shadowLinks = Array.from(shadowRoot.querySelectorAll('a'));
                for (const link of shadowLinks) {
                    const text = link.innerText.toLowerCase();
                    if (text.includes('next chapter') || text.includes('>> next')) {
                        nextUrl = link.href;
                        break;
                    }
                }
            } else {
                bodyText = document.body.innerText;
            }
            
            if (!nextUrl) {
                const links = Array.from(document.querySelectorAll('a'));
                for (const link of links) {
                    const text = link.innerText.toLowerCase();
                    if (text.includes('next chapter') || text.includes('>> next')) {
                        nextUrl = link.href;
                        break;
                    }
                }
            }
            
            return JSON.stringify({
                title: postTitle,
                url: window.location.href,
                content: bodyText,
                nextChapterUrl: nextUrl
            });
        })()
        '''
        
        result = await self.execute_js(tab_id, js_code)
        return json.loads(result)
    
    async def scrape_single_url(self, url: str, tab: dict = None) -> Tuple[List[dict], str]:
        """Scrape má»™t URL, tráº£ vá» chapters vÃ  next URL"""
        close_tab = False
        
        if tab is None:
            # Táº¡o tab má»›i
            tab = await self.create_new_tab()
            close_tab = True
        
        tab_id = tab['id']
        
        try:
            await self.connect_to_tab(tab)
            await self.navigate_and_wait(tab_id, url)
            
            data = await self.extract_chapter_from_tab(tab_id)
            chapters = parse_chapters_from_content(data['content'], data['title'])
            
            return chapters, data.get('nextChapterUrl')
            
        finally:
            # Cleanup
            if tab_id in self.sessions:
                await self.sessions[tab_id]['ws'].close()
                del self.sessions[tab_id]
            
            if close_tab:
                await self.close_tab(tab_id)
    
    async def scrape_urls_parallel(self, urls: List[str]) -> List[dict]:
        """Scrape nhiá»u URLs song song"""
        all_chapters = []
        
        # Chia thÃ nh batches
        for i in range(0, len(urls), self.parallel_tabs):
            batch = urls[i:i + self.parallel_tabs]
            
            print(f"\nğŸš€ Scraping batch {i // self.parallel_tabs + 1} ({len(batch)} URLs song song)...")
            
            # Scrape song song trong batch
            tasks = [self.scrape_single_url(url) for url in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for j, result in enumerate(results):
                if isinstance(result, Exception):
                    print(f"   âŒ Lá»—i vá»›i URL {batch[j]}: {result}")
                else:
                    chapters, _ = result
                    all_chapters.extend(chapters)
                    print(f"   âœ… GÃ³p {len(chapters)} chapter(s)")
            
            # Delay giá»¯a cÃ¡c batch
            if i + self.parallel_tabs < len(urls):
                await asyncio.sleep(self.delay_ms / 1000)
        
        return all_chapters
    
    async def scrape_sequential_with_next(self, start_url: str, count: int) -> List[dict]:
        """Scrape tuáº§n tá»± theo link Next Chapter - nhÆ°ng tá»‘i Æ°u hÆ¡n"""
        all_chapters = []
        current_url = start_url
        
        # Láº¥y tab hiá»‡n cÃ³ hoáº·c táº¡o má»›i
        tabs = await self.get_tabs()
        kofi_tab = None
        for tab in tabs:
            if tab.get('type') == 'page' and 'ko-fi' in tab.get('url', ''):
                kofi_tab = tab
                break
        
        if not kofi_tab:
            kofi_tab = tabs[0] if tabs else await self.create_new_tab()
        
        tab_id = kofi_tab['id']
        
        try:
            await self.connect_to_tab(kofi_tab)
            
            for i in range(count):
                print(f"\nğŸ“– [{i + 1}/{count}] Scraping: {current_url[:60]}...")
                
                # Navigate vÃ  Ä‘á»£i
                loaded = await self.navigate_and_wait(tab_id, current_url)
                if not loaded:
                    print("   âš ï¸ Page load timeout, thá»­ extract anyway...")
                
                # Extract
                data = await self.extract_chapter_from_tab(tab_id)
                chapters = parse_chapters_from_content(data['content'], data['title'])
                
                all_chapters.extend(chapters)
                
                for ch in chapters:
                    print(f"   âœ… Chapter {ch['id']}: {ch['title'][:40]}...")
                
                # Next URL
                if data.get('nextChapterUrl') and i < count - 1:
                    current_url = data['nextChapterUrl']
                    # Delay ngáº¯n
                    await asyncio.sleep(self.delay_ms / 1000)
                else:
                    if i < count - 1:
                        print("   âš ï¸ KhÃ´ng tÃ¬m tháº¥y link Next Chapter")
                    break
                    
        finally:
            if tab_id in self.sessions:
                await self.sessions[tab_id]['ws'].close()
        
        return all_chapters


def clean_text(text: str) -> str:
    """LÃ m sáº¡ch text"""
    lines = text.split('\n')
    
    skip_patterns = [
        r'^>> Next Chapter',
        r'^<< Previous Chapter',
        r'^Support me',
        r'^Buy me a coffee',
        r'^Ko-fi',
        r'^See all$',
        r'^Terms$',
        r'^Privacy$',
        r'^\d+ comments?$',
        r'^Share$',
        r'^Like$',
        r'^Your page$',
        r'^T$',
        r'^\d+ \w+ \d+$',
        r'^Explore$',
        r'^Notifications$',
        r'^\d{1,2}$',
    ]
    
    cleaned = []
    for line in lines:
        line = line.strip()
        if not line or len(line) < 3:
            continue
        
        skip = False
        for pattern in skip_patterns:
            if re.match(pattern, line, re.IGNORECASE):
                skip = True
                break
        
        if not skip:
            cleaned.append(line)
    
    return '\n\n'.join(cleaned)


def parse_chapters_from_content(content: str, title: str) -> List[dict]:
    """Parse chapters tá»« content"""
    vol_match = re.search(r'Vol\.?\s*(\d+)', title, re.IGNORECASE)
    volume = int(vol_match.group(1)) if vol_match else 1
    
    pattern = r'\[Vol\.\s*\d+\]\s*Chapter\s*(\d+):\s*([^\n]+)'
    matches = list(re.finditer(pattern, content, re.IGNORECASE))
    
    chapters = []
    
    if matches:
        for i, match in enumerate(matches):
            chapter_num = int(match.group(1))
            chapter_title = match.group(2).strip()
            
            start_idx = match.end()
            end_idx = matches[i + 1].start() if i < len(matches) - 1 else len(content)
            chapter_content = clean_text(content[start_idx:end_idx])
            
            chapters.append({
                'id': chapter_num,
                'volume': volume,
                'title': chapter_title,
                'content': chapter_content
            })
    else:
        ch_match = re.search(r'Chapter\s*(\d+)', title, re.IGNORECASE)
        chapter_num = int(ch_match.group(1)) if ch_match else 1
        
        chapters.append({
            'id': chapter_num,
            'volume': volume,
            'title': title,
            'content': clean_text(content)
        })
    
    return chapters


def format_to_xml(chapters: List[dict]) -> str:
    """Format chapters thÃ nh XML"""
    def escape_xml(text):
        return (text
            .replace('&', '&amp;')
            .replace('<', '&lt;')
            .replace('>', '&gt;')
            .replace('"', '&quot;')
            .replace("'", '&apos;'))
    
    xml = '<?xml version="1.0" encoding="UTF-8"?>\n<chapters>\n'
    
    for ch in chapters:
        xml += f'  <chapter number="{ch["id"]}" volume="{ch["volume"]}">\n'
        xml += f'    <title>{escape_xml(ch["title"])}</title>\n'
        xml += f'    <text>{escape_xml(ch["content"])}</text>\n'
        xml += f'  </chapter>\n'
    
    xml += '</chapters>'
    return xml


def save_chapters(chapters: List[dict], output_dir: Path = None) -> Path:
    """LÆ°u chapters ra file"""
    if not chapters:
        return None
    
    chapter_ids = [ch['id'] for ch in chapters]
    min_ch, max_ch = min(chapter_ids), max(chapter_ids)
    
    filename = f"ch{min_ch}_{max_ch}.txt" if min_ch != max_ch else f"ch{min_ch}.txt"
    
    if output_dir:
        output_path = Path(output_dir) / filename
    else:
        output_path = Path(__file__).parent / 'Chapters_Untranslated' / filename
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    xml_content = format_to_xml(chapters)
    output_path.write_text(xml_content, encoding='utf-8')
    
    return output_path


async def main_async(args):
    """Main async function"""
    scraper = FastKofiScraper(
        debug_port=args.port,
        parallel_tabs=args.parallel,
        delay_ms=args.delay
    )
    
    if args.urls:
        # Scrape nhiá»u URLs song song
        print(f"ğŸš€ Parallel scraping {len(args.urls)} URLs (max {args.parallel} Ä‘á»“ng thá»i)...")
        chapters = await scraper.scrape_urls_parallel(args.urls)
    else:
        # Scrape tuáº§n tá»± theo Next Chapter
        print(f"ğŸš€ Sequential scraping tá»« {args.url}, {args.count} bÃ i viáº¿t...")
        chapters = await scraper.scrape_sequential_with_next(args.url, args.count)
    
    if chapters:
        output_path = save_chapters(chapters, args.output)
        print(f"\nâœ… HoÃ n thÃ nh! ÄÃ£ lÆ°u {len(chapters)} chapters vÃ o: {output_path}")
    else:
        print("\nâŒ KhÃ´ng extract Ä‘Æ°á»£c chapter nÃ o.")
    
    return chapters


def main():
    parser = argparse.ArgumentParser(
        description='Ko-fi Fast Scraper - Tá»‘i Æ°u tá»‘c Ä‘á»™ vá»›i parallel processing',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
VÃ­ dá»¥ sá»­ dá»¥ng:
  # Scrape 5 bÃ i tá»« má»™t URL theo link Next Chapter
  python kofi_scraper_fast.py --url "https://ko-fi.com/post/..." --count 5
  
  # Scrape nhiá»u URLs song song
  python kofi_scraper_fast.py --urls "url1" "url2" "url3" --parallel 3
  
  # Äiá»u chá»‰nh delay (máº·c Ä‘á»‹nh 500ms)
  python kofi_scraper_fast.py --url "..." --count 10 --delay 300
        '''
    )
    
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--url', '-u', help='URL báº¯t Ä‘áº§u (dÃ¹ng vá»›i --count)')
    group.add_argument('--urls', nargs='+', help='Danh sÃ¡ch URLs Ä‘á»ƒ scrape song song')
    
    parser.add_argument('--count', '-c', type=int, default=1,
                        help='Sá»‘ bÃ i viáº¿t cáº§n scrape khi dÃ¹ng --url (máº·c Ä‘á»‹nh: 1)')
    parser.add_argument('--output', '-o', help='ThÆ° má»¥c output')
    parser.add_argument('--port', '-p', type=int, default=9222,
                        help='Chrome debugging port (máº·c Ä‘á»‹nh: 9222)')
    parser.add_argument('--parallel', type=int, default=3,
                        help='Sá»‘ tabs cháº¡y song song (máº·c Ä‘á»‹nh: 3)')
    parser.add_argument('--delay', type=int, default=500,
                        help='Delay giá»¯a cÃ¡c batch (ms, máº·c Ä‘á»‹nh: 500)')
    
    args = parser.parse_args()
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         Ko-fi FAST Chapter Scraper (Optimized)             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Äáº£m báº£o Chrome Ä‘ang cháº¡y vá»›i debugging:                    â•‘
â•‘   chrome.exe --remote-debugging-port=9222                  â•‘
â•‘                                                            â•‘
â•‘ Tá»‘i Æ°u: Parallel scraping, Smart wait, Reduced delays     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    asyncio.run(main_async(args))


if __name__ == '__main__':
    main()
